---
title: "Feiyaw__OriginalHomeworkCode_04"
author: "Feiya Wang"
date: "10/23/2019"
output:
  html_document: default
  pdf_document: default
---
***

## Homework 4

***
Introduction: This is the code for [Homework 04](https://fuzzyatelin.github.io/bioanth-stats/homework-04.html), on week 7 of AN597

# Question 1
>Write a simple R function, Z.prop.test(), that can perform one- or two-sample Z-tests for proportion data, using the following guidelines.

1.Your function should take the following arguments: p1 and n1 (no default) representing the estimated proportion and sample size (i.e., based on your sample data); p2 and n2 (both defaulting to NULL) that contain a second sample’s proportion and sample size data in the event of a two-sample test; p0 (no default) as the expected value for the population proportion; and alternative (default “two.sided”) and conf.level (default 0.95), to be used in the same way as in the function t.test().

2.When conducting a two-sample test, it should be p1 that is tested as being smaller or larger than p2 when alternative=“less” or alternative=“greater”, the same as in the use of x and y in the function t.test().

3.The function should perform a one-sample Z-test using p1, n1, and p0 if either p2 or n2 (or both) is NULL.

4.The function should contain a check for the rules of thumb we have talked about (n∗p>5 and n∗(1−p)>5) to ensure the validity of assuming the normal distribution in both the one- and two-sample settings. If this is violated, the function should still complete but it should also print an appropriate warning message.

5.The function should return a list containing the members Z (the test statistic), P (the appropriate p value), and CI (the two-sided CI with respect to “conf.level” around p1 in the case of a one-sample test and around p2-p1 in the case of a two-sample test). For all test alternatives (“two.sided”, “greater”, “less”), calculate symmetric CIs based on quantiles of the normal distribution rather than worrying about calculating single-limit confidence bounds.



```{r}
z.prop.test <-function(p1,n1,p2=NULL,n2=NULL,p0,alternative=c("two.sided", "less", "greater"),conf.level=0.95){
  #seeting function for allthe required default and no default values
  ste=sqrt((p1*(1−p1))/n1)                  
  #standard error of population one.
  a=1-conf.level
  # alpha value
  
  if(is.null(p2) | is.null(n2)){  # if p2 and n2 are null, we do the one-propotion z test.
      if(n1∗p0>5 & n1∗(1−p0)>5){    #check for rule of thumbs
        
        z=(p1-p0)/ste                       
        # z-score calculation
      
        if(alternative=="less"){      # contain if(less), else if(greater), else if(two sided), else(error)
         p <- pnorm(z)
         # p value calculation
         
         lower <- p1 - qnorm(1 - a/2) * ste  
         # 1-alpha/2 each in the upper and lower tails of the distribution
          
         upper <- p1 + qnorm(1 - a/2) * ste 
         # 1-alpha/2 each in the upper and lower tails of the distribution
          
         ci <- c(lower, upper)
         # confidence interval
          
         test=TRUE
         #result of test as a boolean veriable.
        
           if(p< a){        # check if p value is less than alpha value.if(yes), else (no) 
          
         }
           else{
          test=FALSE
          # can reject hypothesis because p value > alpha
        }
      }
    
        else if(alternative=="greater"){
         p <- 1-pnorm(z)
         # p value calculation
         
         lower <- p1 - qnorm(1 - a/2) * ste  
         # (1-alpha)/2 each in the upper and lower tails of the distribution
          
         upper <- p1 + qnorm(1 - a/2) * ste 
         # (1-alpha)/2 each in the upper and lower tails of the distribution
          
         ci <- c(lower, upper)
         # confidence interval
          
         test=TRUE
         #result of test as a boolean veriable.
        
         if(p < a){        # check if p value is less than alpha value.if(yes), else (no) 
        
         }
         else{
        test=FALSE
          # can reject hypothesis because p value > alpha
        }
      }
      
        else if(alternative=="two.sided"){ 
          if(z>0){
          p <- 2*(1-pnorm(z))}
          #p value calculation
          else{
            p<-2*pnorm(z)
          }
        
         lower <- p1 - qnorm(1 - a/2) * ste  
         # (1-alpha)/2 each in the upper and lower tails of the distribution
          
         upper <- p1 + qnorm(1 - a/2) * ste 
         # (1-alpha)/2 each in the upper and lower tails of the distribution
          
         ci <- c(lower, upper)
         # confidence interval
          
         test=TRUE
         #result of test as a boolean veriable.

          if(p< a){        # check if p value is less than alpha value.if(yes), else (no) 
         }
          else{
          test=FALSE
          # can reject hypothesis because p value > alpha
        }
    }
    
        else { #error
       warning("it is a one-proptional z test")
      # error message
    }
        if (test==TRUE){
          s="we cannot reject null hypothesis"
        }
        else{
           s="we can reject the null hypothesis"
        }
        r=c(z,p,ci,s)
        names(r)=c("z","p-value","lower","upper","result")
        return(r)
      }
      else{     # cannot pass rule of thumbs
      warning("cannot assume this normal approximation since it didn't pass rule of thumb check")
      }
  
    }
    
  else if(!(is.null(p2)&is.null(n2))){  "if we have p2,n2 value, we should do two proption z test"
      
      p0=(n1*p1+n2*p2)/(n1+n2)
      # define the pooled propotion
      ste=sqrt(p0*(1−p0)*(1/n1+1/n2))
      # standard error of p1+p2
      if(n1∗p1>5 & n1∗(1−p1)>5 & n2∗p2>5& n2∗(1-p2)>5){            #check for rule of thumbs
        z=(p1-p2)/ste
        # z-score for two propotion z test
        m = p1-p2
        #restate m. 
        if(alternative=="less"){   # contain if(less), else if(greater), else if(two sided), else(error)
          p <- pnorm(z)
          # p value calculation
          
          
         lower <- m - qnorm(1 - a/2) * ste  
         # (1-alpha)/2 each in the upper and lower tails of the distribution
          
         upper <- m + qnorm(1 - a/2) * ste 
         # (1-alpha)/2 each in the upper and lower tails of the distribution
          
         ci <- c(lower, upper)
         # confidence interval
          
         test=TRUE
         #result of test as a boolean veriable.
        
           if(p< a){        # check if p value is less than alpha value.if(yes), else (no)
         }
           else{
          test=FALSE
          # can reject hypothesis because p value > alpha
        }
      }
    
        else if(alternative=="greater"){
            p <- 1-pnorm(z)
            # p value calculation
            
         lower <- m - qnorm(1 - a/2) * ste  
         # (1-alpha)/2 each in the upper and lower tails of the distribution
          
         upper <- m + qnorm(1 - a/2) * ste 
         # (1-alpha)/2 each in the upper and lower tails of the distribution
          
         ci <- c(lower, upper)
         # confidence interval
          
         test=TRUE
         #result of test as a boolean veriable.
          
        
           if(p< a){        # check if p value is less than alpha value.if(yes), else (no) 
  
         }
           else{
         
              test=FALSE
          # can reject hypothesis because p value > alpha
        }
      }
      
        else if(alternative=="two.sided"){ 
          if(z>0){
          p <- 2*(1-pnorm(z))}
          #p value calculation
          else{
            p<-2*pnorm(z)
          }
          
         lower <- m - qnorm(1 - a/2) * ste  
         # (1-alpha)/2 each in the upper and lower tails of the distribution
          
         upper <- m + qnorm(1 - a/2) * ste 
         # (1-alpha)/2 each in the upper and lower tails of the distribution
          
         ci <- c(lower, upper)
         # confidence interval
          
         test=TRUE
         #result of test as a boolean veriable.
    
        
           if(p< a){        # check if p value is less than alpha value.if(yes), else (no) 
         }
           else{
          test=FALSE
          # can reject hypothesis because p value > alpha
        }
    }
    
        else { #error
       warning("it is a one-proptional z test")
      # error message
    }
      
      }
      else{    # cannot pass the check for rule of thumbs
        warning("cannot assume this normal approximation since it didn't pass rule of thumb check")
      }
      if (test==TRUE){
          s="we cannot reject null hypothesis"
        }
        else{
           s="we can reject the null hypothesis"
        }
        r=c(z,p,ci,s)
        names(r)=c("z","p-value","lower","upper","result")
        return(r)
      }
    
  
  else {
        warning("cannot assume this normal approximation since it didn't pass rule of thumb check")
    }
  }

```

Here is the test of the z.prop.function we write.

Check for one samle z proportion test:
```{r}
prop.test(520, 1000, p = 0.5, alternative = "two.sided",
          correct = TRUE)
# standard equation from R. X is number of sucess, n is number of total trial. we use x=75 and n=100, p=0.5.
z.prop.test(p1=0.520,n1=1000,p0=0.5,alternative="two.sided",conf.level=0.95)
# our equation with the same data.
```

Got same reuslts as the stadard function

Two-sample Z proportion test:
```{r}
prop.test(x = c(490, 400), n = c(500, 500))
#standr equation. sample 1= 490 sucess out of 500, sample 2=400 sucess out of 500 
z.prop.test(p1=0.98,n1=500,p2=0.80,n2=500,p0=0.5,alternative="two.sided",conf.level=0.95)
#same data for our euqation. p0 can be any value sinc ethe functio will calculate p0 instead.
```

Got same results as the standard function.

Our Function is successful!



# Question 2

>The dataset from Kamilar and Cooper has in it a large number of variables related to life history and body size. For this exercise, the end aim is to fit a simple linear regression model to predict longevity (MaxLongevity_m) measured in months from species’ brain size (Brain_Size_Species_Mean) measured in grams. Do the following for both longevity~brain size and log(longevity)~log(brain size):

```{r}
library (curl)
#install package curl before this step to load a file from server.
f <- curl("https://raw.githubusercontent.com/fuzzyatelin/fuzzyatelin.github.io/master/AN597_Fall19/KamilarAndCooperData.csv")
#load the file in varibale f
d <- read.csv(f, header = TRUE, sep = ",", stringsAsFactors = FALSE)
# read file in d as a dataframe.

```


>1.Fit the regression model and, using {ggplot2}, produce a scatterplot with the fitted line superimposed upon the data. Append the the fitted model equation to your plot (HINT: use the function geom_text()).

```{r}
df<-as.data.frame(cbind(d$Brain_Size_Species_Mean,d$MaxLongevity_m))
#get data frame from preivous data

df<-na.omit(df)
#delete all the NA value

colnames(df)<-c("Brain_Size_Species_Mean","MaxLongevity_m")

library(ggplot2)

p1 <- ggplot(data=df, aes(x=Brain_Size_Species_Mean,y=MaxLongevity_m))
# build a ggplot object

p1 <- p1 + geom_point()+xlim(0,500)+ylim(0,750)
# make a scatterplot

p1 <- p1 + geom_smooth(method="lm", fullrange=TRUE)
# add a regression line 

m1<-lm(data=df,MaxLongevity_m ~ Brain_Size_Species_Mean)
# make the model of two variable

t1 <- unlist(m1$coefficients)
# unlist m to get coeffient

beta0<-round(t1[1],digits = 3)
#round beta 1 to 3 decimal place

beta1<-round(t1[2],digits = 3)
#round beta 0 to 3 decimal place

soe1 <- paste("y=",as.character(beta1),"x+",as.character(beta0))
#make the regression equation as a string

p1<-p1+annotate("text", label = soe1, x = 400, y = 200, size = 4, colour = "black")
#add it to the ggplot

r1<-log(df$MaxLongevity_m)

r2<-log(df$Brain_Size_Species_Mean)

df2<-as.data.frame(cbind(r2,r1))
#have a data frame for next question

p2 <- ggplot(data=df, aes(x=r2,y=r1))+xlab("log(Brain_Size_Species_Mean)")+ylab("log(MaxLongevity_m)")
# build a ggplot object

p2 <- p2 + geom_point() 
# make a scatterplot

p2 <- p2 + geom_smooth(method="lm", fullrange=TRUE)
# add a regression line 

m2<-lm(data=df,r1 ~ r2)
# make the model of two variable

t2 <- unlist(m2$coefficients)
# unlist m to get coeffient

beta00<-round(t2[1],digits = 3)
#round beta 1 to 3 decimal place

beta11<-round(t2[2],digits = 3)
#round beta 0 to 3 decimal place

soe2<-paste("y=",as.character(beta11),"x+",as.character(beta00))
#make the regression equation as a string

p2<-p2+ annotate("text", label = soe2, x = 5.0, y = 7.0, size = 4, colour = "black")
#addd it to ggplot

library(ggpubr)
figure1 <- ggarrange(p1,p2,
                    ncol = 2, nrow = 1)
# use this unction to combine 2 ggplot into one figure and label it to make it look nice

annotate_figure(
  figure1,
  top = text_grob("Scatterplots for regression model  ",
                  color = "black", face = "bold", size = 14)
  )
```

>2.Identify and interpret the point estimate of the slope (β1), as well as the outcome of the test associated with the hypotheses H0: β1 = 0; HA: β1 ≠ 0. Also, find a 90 percent CI for the slope (β1) parameter.

For regression model of longevity (MaxLongevity_m)from species’ brain size (Brain_Size_Species_Mean):
```{r}
t1 <- coef(summary(m1))
#summary of m1 we use as the regression model
t1 <- data.frame(unlist(t1))
#unlist it
colnames(t1) <- c("Est", "SE", "t", "p")
#name the columns. find t and p
t1$p[2]
#show the p values of t test for slope(beta1) only 
```

Since p<0.05, we reject the null hypothesis. Which means that beta 1 is not equal to zero.

For 90% confident interval:
```{r}
ci1 <- confint(m1, level = 0.90)
#confident interval in m1
ci1[2,]
#show only beta1 confident interval.
```



For regression model of log of longevity [log(MaxLongevity_m)]from log of species’ brain size [log(Brain_Size_Species_Mean)]:
```{r}
t2 <- coef(summary(m2))
#summary of m1 we use as the regression model
t2 <- data.frame(unlist(t2))
#unlist it
colnames(t2) <- c("Est", "SE", "t", "p")
#name the columns. find t and p
t2$p[2]
#show the p values of t test for slope(beta1) only 
```

Since p<0.05, we reject the null hypothesis. Which means that beta 1 is not equal to zero.

For 90% confident interval:
```{r}
ci2 <- confint(m2, level = 0.90)
#confident interval in m1
ci2[2,]
#show only beta1 confident interval.
```


>3.Using your model, add lines for the 90 percent confidence and prediction interval bands on the plot and add a legend to differentiate between the lines.


```{r,fig.height=5,fig.width=10}
ci_1 <- predict(m1, newdata = df, interval = "confidence", level = 0.90)
#confidence interval
pi_1 <- predict(m1, newdata = df, interval = "prediction", level = 0.90)
#pprediction interval
df <- data.frame(cbind(df, ci_1,pi_1))
#combine as a dafa frame
names(df) <- c("x", "y", "CIfit", "CIlwr", "CIupr","PIfit", "PIlwr", "PIupr")
#name different column

g1 <- ggplot(data = df, aes(x = x, y = y))
g1 <- g1 + geom_point(alpha = 1/2)
g1 <- g1 +
  geom_line(aes(y = CIfit , colour = "fit_line")) +
  geom_line(aes(y = CIlwr, colour = "confident interval")) +
  geom_line(aes(y = CIupr, colour = "confident interval")) +
  geom_line(aes(y = PIlwr, colour = "prediction"))+
  geom_line(aes(y = PIupr, colour = "prediction"))+
  scale_colour_manual("", 
                      values = c("fit_line"="black", "confident interval"="blue", 
                                 "prediction"="red")) +
  xlab("Brain_Size_Species_Mean")+ylab("MaxLongevity_m")


ci_2 <- predict(m2, newdata=df2, interval = "confidence", level = 0.90)
#confident interval
pi_2 <- predict(m2, newdata = df2, interval = "prediction", level = 0.90)
#predict intrval
df_2 <- data.frame(cbind(df2, ci_2,pi_2))
#combine a new data frame
names(df_2) <- c("x", "y", "CIfit", "CIlwr", "CIupr","PIfit", "PIlwr", "PIupr")
#rename the columns.

g2 <- ggplot(data = df_2, aes(x = x, y = y))+xlab("log(Brain_Size_Species_Mean)")+ylab("log(MaxLongevity_m)")
g2 <- g2 + geom_point(alpha = 1/2)
g2 <- g2+ geom_line(aes(y = CIfit , colour = "fit_line")) +
  geom_line(aes(y = CIlwr, colour = "confident interval")) +
  geom_line(aes(y = CIupr, colour = "confident interval")) +
  geom_line(aes(y = PIlwr, colour = "prediction"))+
  geom_line(aes(y = PIupr, colour = "prediction"))+
  scale_colour_manual("", 
                      values = c("fit_line"="black", "confident interval"="blue", 
                               "prediction"="red"))

library(ggpubr)
figure2 <- ggarrange(g1,g2,
                    ncol = 2, nrow = 1)
# use this unction to combine 2 ggplot into one figure and label it to make it look nice

annotate_figure(
  figure2,
  top = text_grob("lines for the 90 percent confidence and prediction interval",
                  color = "black", face = "bold", size = 14)
  )

```


>4.Produce a point estimate and associated 90 percent PI for the longevity of a species whose brain weight is 800 gm. Do you trust the model to predict observations accurately for this value of the explanatory variable? Why or why not?
Looking at your two models, which do you think is better? Why?

```{r}
pi1 <- predict(m1, newdata = data.frame(Brain_Size_Species_Mean = 800), interval = "prediction", 
    level = 0.90) 
pi1
#for the model of MaxLongevity_m from Brain_Size_Species_Mean 
pi2<-predict(m2, newdata = data.frame(r2 = log(800)), interval = "prediction", 
    level = 0.90) 
pi2
#for the model of log(longevity)~log(brain size)

```


The log()model is better since the line of regression is have less outliers and the Confident interval line and prediction line are more uniform.




# five Challage
1. i have a hard time to figure out the correct version of z prop function, the logic of if and else need to be very careful. i miss place some of if ,else in original homework.
2. I also spend sometimes on graph the ggplot of the confidence and predict line. Since i didn't reove NA before, my graph have a wired looking line. it fixed until i remove all NA.
3. think about puting legend is also hard. I search on google and find a method to solve it.
4. There is no peer commnetary for my work. i have to figure out my own.



```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

